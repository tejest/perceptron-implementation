# -*- coding: utf-8 -*-
"""perceptron using gauss().ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hn0YFUobB8yx6uLqmRHXqJQedHk6ALak
"""

import random
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


def train_test_split(X, y, test_size=0.2, stratify=None, random_seed=None):

    if random_seed is not None:
        np.random.seed(random_seed)

    X = np.asarray(X)
    y = np.asarray(y)

    if stratify is None:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        n_test = int(len(indices) * test_size)
        test_idx = indices[:n_test]
        train_idx = indices[n_test:]
    else:
        classes = np.unique(stratify)
        train_idx, test_idx = [], []
        for c in classes:
            c_idx = np.where(stratify == c)[0]
            np.random.shuffle(c_idx)
            n_test_c = int(len(c_idx) * test_size)
            test_idx.extend(c_idx[:n_test_c])
            train_idx.extend(c_idx[n_test_c:])
        test_idx = np.array(test_idx)
        train_idx = np.array(train_idx)
        np.random.shuffle(test_idx)
        np.random.shuffle(train_idx)

    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]


class ManualScaler:

    def __init__(self):
        self.mean_ = None
        self.std_ = None

    def fit(self, X):
        X = np.asarray(X, dtype=float)
        self.mean_ = np.mean(X, axis=0)
        self.std_ = np.std(X, axis=0)

        self.std_ = np.where(self.std_ == 0, 1.0, self.std_)
        return self

    def transform(self, X):
        if self.mean_ is None or self.std_ is None:
            raise ValueError("Scaler must be fitted before transforming")
        X = np.asarray(X, dtype=float)
        return (X - self.mean_) / self.std_

    def fit_transform(self, X):
        return self.fit(X).transform(X)


class Perceptron:

    def __init__(self, learning_rate=0.01, max_epochs=100, random_state=None,
                 show_boundary_evolution=True, verbose=True):
        self.learning_rate = learning_rate
        self.max_epochs = max_epochs
        self.random_state = random_state
        self.show_boundary_evolution = show_boundary_evolution
        self.verbose = verbose
        self.weights = None
        self.bias = None
        self.errors_history = []
        self.accuracy_history = []
        self.weights_history = []
        self.bias_history = []
        self.scaler = None
        self.X_train_orig = None
        self.is_fitted = False

    def fit(self, X_scaled, y, X_orig=None, scaler=None):

        if self.random_state is not None:
            np.random.seed(self.random_state)

        X = np.asarray(X_scaled, dtype=float)
        y = np.asarray(y, dtype=int)

        if len(X) != len(y):
            raise ValueError("X and y must have the same number of samples")

        n_features = X.shape[1]


        self.scaler = scaler
        self.X_train_orig = X_orig if X_orig is not None else X


        self.weights = np.random.uniform(0,0.1,n_features)
        self.bias = np.random.uniform(-0.1, 0.1)


        self.is_fitted = True


        self.errors_history = []
        self.accuracy_history = []
        self.weights_history = [self.weights.copy()]
        self.bias_history = [self.bias]

        if self.verbose:
            print(f"Initial: weights={self.weights}, bias={self.bias:.4f}")


        if self.show_boundary_evolution and scaler is not None:
            self._plot_boundary_evolution(self.X_train_orig, y, epoch=0,
                                        before_training=True, scaler=scaler)

        for epoch in range(1, self.max_epochs + 1):
            errors = 0
            indices = np.random.permutation(len(X))

            for idx in indices:
                x_i = X[idx]
                y_i = y[idx]


                linear_output = np.dot(x_i, self.weights) + self.bias
                prediction = self._activation(linear_output)


                if prediction != y_i:
                    error = y_i - prediction
                    self.weights += self.learning_rate * error * x_i
                    self.bias += self.learning_rate * error
                    errors += 1


            accuracy = (len(X) - errors) / len(X)
            self.errors_history.append(errors)
            self.accuracy_history.append(accuracy)
            self.weights_history.append(self.weights.copy())
            self.bias_history.append(self.bias)

            if self.show_boundary_evolution and scaler is not None:
              self._plot_boundary_evolution(self.X_train_orig, y, epoch, scaler=scaler)


            if self.verbose and (epoch % 20 == 0 or epoch <= 10 or errors == 0):
                print(f"Epoch {epoch:3d}: Errors={errors:3d}, Accuracy={accuracy:.4f}")


            if errors == 0:
                if self.verbose:
                    print(f"Converged at epoch {epoch}!")
                if self.show_boundary_evolution and scaler is not None:
                    self._plot_boundary_evolution(self.X_train_orig, y, epoch,
                                                final=True, scaler=scaler)
                break

        return self

    def _plot_boundary_evolution(self, X_orig, y, epoch, final=False,
                                before_training=False, scaler=None):

        if scaler is None:
            return

        plt.figure(figsize=(10, 8))


        margin = 1.0
        x_min, x_max = X_orig[:, 0].min() - margin, X_orig[:, 0].max() + margin
        y_min, y_max = X_orig[:, 1].min() - margin, X_orig[:, 1].max() + margin
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                             np.arange(y_min, y_max, 0.1))


        mesh_points_orig = np.c_[xx.ravel(), yy.ravel()]
        mesh_points_scaled = scaler.transform(mesh_points_orig)
        Z = self.decision_function(mesh_points_scaled).reshape(xx.shape)


        plt.contour(xx, yy, Z, levels=[0], colors='black', linestyles='-', linewidths=2)
        plt.contourf(xx, yy, Z, levels=[-np.inf, 0, np.inf],
                    colors=['lightcoral', 'lightblue'], alpha=0.4)


        class_0_mask = (y == 0)
        class_1_mask = (y == 1)
        plt.scatter(X_orig[class_0_mask, 0], X_orig[class_0_mask, 1],
                   c='red', marker='o', s=50, label='Class 0',
                   alpha=0.7, edgecolor='darkred')
        plt.scatter(X_orig[class_1_mask, 0], X_orig[class_1_mask, 1],
                   c='blue', marker='s', s=50, label='Class 1',
                   alpha=0.7, edgecolor='darkblue')


        if before_training:
            accuracy_text = "N/A"
        elif epoch <= len(self.accuracy_history):
            accuracy_text = f"{self.accuracy_history[epoch-1]:.3f}"
        else:
            accuracy_text = "N/A"

        title_suffix = " (FINAL)" if final else ""
        plt.title(f'Decision Boundary - Epoch {epoch}{title_suffix}\n'
                  f'w1={self.weights[0]:.3f}, w2={self.weights[1]:.3f}, '
                  f'bias={self.bias:.3f}, Accuracy={accuracy_text}')

        plt.xlabel('Feature x1')
        plt.ylabel('Feature x2')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    def _activation(self, x):

        return 1 if x >= 0 else 0

    def predict(self, X):

        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")

        X = np.asarray(X, dtype=float)
        linear_output = np.dot(X, self.weights) + self.bias
        return np.array([self._activation(x) for x in linear_output])

    def decision_function(self, X):

        if not self.is_fitted:
            raise ValueError("Model must be fitted before using decision function")

        X = np.asarray(X, dtype=float)
        return np.dot(X, self.weights) + self.bias

    def score(self, X, y):

        predictions = self.predict(X)
        return np.mean(predictions == y)


def generate_two_class_dataset(n_samples_per_class=200, random_seed=None):

    if random_seed is not None:
        random.seed(random_seed)
        np.random.seed(random_seed)


    mu1_class0, sigma1_class0 = 4.0, 1.0
    mu2_class0, sigma2_class0 = 3.0, 1.2


    mu1_class1, sigma1_class1 = 7, 1.1
    mu2_class1, sigma2_class1 = 8, 0.9


    class0_x1 = [random.gauss(mu1_class0, sigma1_class0) for _ in range(n_samples_per_class)]
    class0_x2 = [random.gauss(mu2_class0, sigma2_class0) for _ in range(n_samples_per_class)]
    class0_labels = [0] * n_samples_per_class


    class1_x1 = [random.gauss(mu1_class1, sigma1_class1) for _ in range(n_samples_per_class)]
    class1_x2 = [random.gauss(mu2_class1, sigma2_class1) for _ in range(n_samples_per_class)]
    class1_labels = [1] * n_samples_per_class


    x1_all = class0_x1 + class1_x1
    x2_all = class0_x2 + class1_x2
    labels_all = class0_labels + class1_labels


    dataset = pd.DataFrame({'x1': x1_all, 'x2': x2_all, 'class': labels_all})
    return dataset.sample(frac=1).reset_index(drop=True)


def plot_all_decision_boundaries(model, X_orig, y, scaler):

    if not model.weights_history or scaler is None:
        print("No training history or scaler available for plotting")
        return

    plt.figure(figsize=(12, 10))


    margin = 1.0
    x_min, x_max = X_orig[:, 0].min() - margin, X_orig[:, 0].max() + margin
    y_min, y_max = X_orig[:, 1].min() - margin, X_orig[:, 1].max() + margin
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    mesh_points_orig = np.c_[xx.ravel(), yy.ravel()]
    mesh_points_scaled = scaler.transform(mesh_points_orig)


    class_0_mask = (y == 0)
    class_1_mask = (y == 1)
    plt.scatter(X_orig[class_0_mask, 0], X_orig[class_0_mask, 1],
               c='red', marker='o', s=50, label='Class 0',
               alpha=0.7, edgecolor='darkred', zorder=5)
    plt.scatter(X_orig[class_1_mask, 0], X_orig[class_1_mask, 1],
               c='blue', marker='s', s=50, label='Class 1',
               alpha=0.7, edgecolor='darkblue', zorder=5)

    n_boundaries = len(model.weights_history)
    colors = plt.cm.viridis(np.linspace(0.2, 1.0, n_boundaries))

    epochs_to_plot = list(range(n_boundaries))


    epochs_to_plot = sorted(list(set(epochs_to_plot)))

    for idx, epoch_idx in enumerate(epochs_to_plot):
        weights = model.weights_history[epoch_idx]
        bias = model.bias_history[epoch_idx]

        Z = (np.dot(mesh_points_scaled, weights) + bias).reshape(xx.shape)

        if epoch_idx == 0:
            linestyle = '--'
            linewidth = 2
            alpha = 0.8
            label_suffix = " (Initial)"
        elif epoch_idx == n_boundaries - 1:
            linestyle = '-'
            linewidth = 3
            alpha = 1.0
            label_suffix = " (Final)"
        else:
            linestyle = '-'
            linewidth = 1.5
            alpha = 0.6
            label_suffix = ""


        color_idx = int((epoch_idx / (n_boundaries - 1)) * (len(colors) - 1))
        plt.contour(xx, yy, Z, levels=[0], colors=[colors[color_idx]],
                   linestyles=linestyle, linewidths=linewidth, alpha=alpha,
                   label=f'Epoch {epoch_idx}{label_suffix}')

    final_weights = model.weights_history[-1]
    final_bias = model.bias_history[-1]
    Z_final = (np.dot(mesh_points_scaled, final_weights) + final_bias).reshape(xx.shape)
    plt.contourf(xx, yy, Z_final, levels=[-np.inf, 0, np.inf],
                colors=['lightcoral', 'lightblue'], alpha=0.2)

    plt.title('Decision Boundary Evolution Throughout Training\n'
              f'Total Epochs: {n_boundaries-1}, Boundaries Shown: {len(epochs_to_plot)}',
              fontsize=14)
    plt.xlabel('Feature x1', fontsize=12)
    plt.ylabel('Feature x2', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()

    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis,
                              norm=plt.Normalize(vmin=0, vmax=n_boundaries-1))
    sm.set_array([])
    cbar = plt.colorbar(sm, ax=plt.gca(), shrink=0.8)
    cbar.set_label('Epoch Number', fontsize=10)

    plt.show()

    print(f"Decision Boundary Evolution Summary:")
    print(f"• Total epochs trained: {n_boundaries-1}")
    print(f"• Boundaries plotted: {len(epochs_to_plot)}")
    print(f"• Epochs shown: {epochs_to_plot}")
    if len(model.errors_history) > 0:
        final_errors = model.errors_history[-1]
        print(f"• Final training errors: {final_errors}")
        print(f"• Convergence: {'Yes' if final_errors == 0 else 'No'}")





def predict_new_points(model, scaler, n_points=8, random_seed=None):

    if random_seed is not None:
        random.seed(random_seed)

    print(f"\nPredicting New Random Points:")
    print("=" * 60)


    new_points = []
    for i in range(n_points):
        x1 = random.gauss(4.0, 2.0)
        x2 = random.gauss(5.0, 2.5)
        new_points.append([x1, x2])

    new_points = np.array(new_points)

    new_points_scaled = scaler.transform(new_points)
    decision_values = model.decision_function(new_points_scaled)
    predictions = model.predict(new_points_scaled)

    print(f"{'Point':<5} {'x1':<8} {'x2':<8} {'Linear Value':<12} {'Position':<15} {'Predicted Class':<15}")
    print("-" * 75)

    for i, (point, decision_val, pred) in enumerate(zip(new_points, decision_values, predictions)):
        x1, x2 = point
        if decision_val > 0:
            position = "Above line"
        elif decision_val < 0:
            position = "Below line"
        else:
            position = "On line"
        class_label = f"Class {pred}"
        print(f"{i+1:<5} {x1:<8.3f} {x2:<8.3f} {decision_val:<12.4f} {position:<15} {class_label:<15}")

    plt.figure(figsize=(12, 8))

    h = 0.1
    x_min, x_max = new_points[:, 0].min() - 2, new_points[:, 0].max() + 2
    y_min, y_max = new_points[:, 1].min() - 2, new_points[:, 1].max() + 2
    x_min, x_max = min(x_min, -1), max(x_max, 9)
    y_min, y_max = min(y_min, 0), max(y_max, 10)

    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    mesh_points = np.c_[xx.ravel(), yy.ravel()]
    mesh_scaled = scaler.transform(mesh_points)
    Z = model.decision_function(mesh_scaled).reshape(xx.shape)

    plt.contour(xx, yy, Z, levels=[0], colors='black', linestyles='-', linewidths=3)
    plt.contourf(xx, yy, Z, levels=[-np.inf, 0, np.inf],
                colors=['lightcoral', 'lightblue'], alpha=0.3)

    class0_mask = (predictions == 0)
    class1_mask = (predictions == 1)

    if np.any(class0_mask):
        plt.scatter(new_points[class0_mask, 0], new_points[class0_mask, 1],
                   c='darkred', marker='o', s=100, label='Predicted Class 0',
                   alpha=0.8, edgecolor='black', linewidth=2)

    if np.any(class1_mask):
        plt.scatter(new_points[class1_mask, 0], new_points[class1_mask, 1],
                   c='darkblue', marker='s', s=100, label='Predicted Class 1',
                   alpha=0.8, edgecolor='black', linewidth=2)

    for i, (x1, x2) in enumerate(new_points):
        plt.annotate(f'{i+1}', (x1, x2), xytext=(5, 5), textcoords='offset points',
                    fontsize=10, fontweight='bold', color='white',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))

    plt.xlabel('Feature x1', fontsize=12)
    plt.ylabel('Feature x2', fontsize=12)
    plt.title('Predictions on New Random Points\nDecision Boundary Classification', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)

    explanation = ("Points above the line (positive linear value) → Class 1\n"
                  "Points below the line (negative linear value) → Class 0")
    plt.text(0.02, 0.02, explanation, transform=plt.gca().transAxes, fontsize=10,
            verticalalignment='bottom',
            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))

    plt.tight_layout()
    plt.show()

    return new_points, predictions, decision_values


def plot_dataset_only(data):

    plt.figure(figsize=(8, 6))

    class_0 = data[data['class'] == 0]
    class_1 = data[data['class'] == 1]

    plt.scatter(class_0['x1'], class_0['x2'], c='red', marker='o', s=50,
                label='Class 0', alpha=0.7, edgecolor='darkred')
    plt.scatter(class_1['x1'], class_1['x2'], c='blue', marker='s', s=50,
                label='Class 1', alpha=0.7, edgecolor='darkblue')

    plt.title("Generated Dataset", fontsize=14)
    plt.xlabel("Feature x1", fontsize=12)
    plt.ylabel("Feature x2", fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def main():

    print("=" * 60)
    print("PERCEPTRON CLASSIFIER DEMONSTRATION")
    print("=" * 60)

    data = generate_two_class_dataset(n_samples_per_class=150, random_seed=None)
    print(f"Dataset shape: {data.shape}")
    print(f"Class distribution:\n{data['class'].value_counts()}")

    print(f"\nGenerated Dataset:")
    print(data.to_string(index=False))

    plot_dataset_only(data)

    X = data[['x1', 'x2']].values
    y = data['class'].values

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_seed=None
    )

    scaler = ManualScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print(f"\nTraining set size: {len(X_train)}")
    print(f"Test set size: {len(X_test)}")

    print("\nTraining Perceptron...")
    perceptron = Perceptron(
        learning_rate=0.1,
        max_epochs=100,
        random_state=None,
        show_boundary_evolution=True
    )

    perceptron.fit(X_train_scaled, y_train, X_orig=X_train, scaler=scaler)

    print(f"\nFinal Parameters:")
    print(f"Weight 1 (w1): {perceptron.weights[0]:8.4f}")
    print(f"Weight 2 (w2): {perceptron.weights[1]:8.4f}")
    print(f"Bias (b):       {perceptron.bias:8.4f}")
    print(f"Decision boundary: {perceptron.weights[0]:.3f}*x1 + {perceptron.weights[1]:.3f}*x2 + {perceptron.bias:.3f} = 0")

    train_accuracy = perceptron.score(X_train_scaled, y_train)
    test_accuracy = perceptron.score(X_test_scaled, y_test)
    print(f"\nModel Performance:")
    print(f"Training Accuracy: {train_accuracy:.4f}")
    print(f"Test Accuracy:     {test_accuracy:.4f}")

    print("\nGenerating decision boundary evolution plot...")
    plot_all_decision_boundaries(perceptron, X_train, y_train, scaler)

    print("\n" + "=" * 60)
    print("TESTING ON NEW RANDOM POINTS")
    print("=" * 60)
    predict_new_points(perceptron, scaler, n_points=8, random_seed=None)

    print(f"\nDecision Boundary Explanation:")
    print(f"• Equation: {perceptron.weights[0]:.3f}*x1 + {perceptron.weights[1]:.3f}*x2 + {perceptron.bias:.3f} = 0")
    print(f"• If value > 0 → Class 1 (above/right of boundary)")
    print(f"• If value < 0 → Class 0 (below/left of boundary)")
    print(f"• If value = 0 → On the decision boundary")



if __name__ == "__main__":
    main()