# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pgG6eXxPsUfkmZyw_r1tTujLhoJFw9h5
"""

import random
import pandas as pd
import matplotlib.pyplot as plt


def zeros(size):
    result = []
    i = 0
    while i < size:
        result.append(0.0)
        i += 1
    return result

def dot(a, b):
    result = 0.0
    i = 0
    while i < len(a):
        result += a[i] * b[i]
        i += 1
    return result

def step_function(x):
    if x >= 0:
        return 1
    else:
        return 0


def generate_random_dataset(n_samples=120, feature_range=(0,50)):
    w1 = random.uniform(-1,1)
    w2 = random.uniform(-1,1)
    b = random.uniform(-10,10)

    X = []
    y = []
    margin = 5

    i = 0
    while i < n_samples:
        x1 = random.randint(feature_range[0], feature_range[1])
        cls = random.choice([0,1])
        if abs(w2) > 1e-10:
            y_line = -(w1*x1 + b)/w2
            if cls == 1:
                x2 = y_line + random.uniform(margin, margin+10)
            else:
                x2 = y_line - random.uniform(margin, margin+10)
        else:
            if cls == 1:
                x2 = random.randint(feature_range[0], feature_range[1])
                x1 += random.uniform(margin, margin+10)
            else:
                x2 = random.randint(feature_range[0], feature_range[1])
                x1 -= random.uniform(margin, margin+10)
        X.append([x1, x2])
        y.append(cls)
        i += 1

    # Create a DataFrame for tabular display
    df = pd.DataFrame(X, columns=["Feature 1 (x1)", "Feature 2 (x2)"])
    df["Output (y)"] = y

    return X, y, [w1, w2], b, df


class EnhancedPerceptron:
    def __init__(self, lr=0.1, epochs=50):
        self.lr = lr
        self.epochs = epochs
        self.weights = None
        self.bias = 0.0
        self.epsilon = 1e-10
        self.history = []

    def fit(self, X, y):
        n_samples = len(X)
        n_features = len(X[0])
        self.weights = zeros(n_features)
        self.bias = 0.0
        self.history.append([list(self.weights), self.bias])

        epoch = 1
        while epoch <= self.epochs:
            errors = 0
            i = 0
            while i < n_samples:
                linear_output = dot(self.weights, X[i]) + self.bias
                y_pred = step_function(linear_output)
                error = y[i] - y_pred
                if error != 0:
                    errors += 1
                    j = 0
                    while j < n_features:
                        self.weights[j] += self.lr * error * X[i][j]
                        j += 1
                    self.bias += self.lr * error
                i += 1

            self.history.append([list(self.weights), self.bias])

            correct = 0
            i = 0
            while i < n_samples:
                pred = step_function(dot(self.weights, X[i]) + self.bias)
                if pred == y[i]:
                    correct += 1
                i += 1
            accuracy = correct / n_samples

            # Print the equation of the decision boundary
            equation = self.get_decision_boundary_equation()
            print("Epoch", epoch, "Weights:", [round(w, 3) for w in self.weights], "Bias:", round(self.bias, 3), "Accuracy:", round(accuracy, 3))
            print("  Decision Boundary:", equation)

            self.plot_epoch(X, y, epoch)

            if errors == 0 or accuracy >= 1.0:
                print("\nConverged at epoch", epoch)
                break
            epoch += 1

    def get_decision_boundary_equation(self):
        w1, w2 = self.weights[0], self.weights[1]
        b = self.bias
        if abs(w1) < self.epsilon and abs(w2) < self.epsilon:
            return "Undefined (weights are zero)"

        equation_parts = []
        if abs(w1) > self.epsilon:
            if abs(w1 - 1) < self.epsilon:
                equation_parts.append("x1")
            elif abs(w1 + 1) < self.epsilon:
                equation_parts.append("-x1")
            else:
                equation_parts.append(f"{w1:.3f}*x1")

        if abs(w2) > self.epsilon:
            sign = "+" if w2 > 0 else ""
            if abs(w2 - 1) < self.epsilon:
                equation_parts.append(f"{sign}x2")
            elif abs(w2 + 1) < self.epsilon:
                equation_parts.append("-x2")
            else:
                equation_parts.append(f"{sign}{w2:.3f}*x2")

        if abs(b) > self.epsilon:
            sign = "+" if b > 0 else ""
            equation_parts.append(f"{sign}{b:.3f}")

        equation = " ".join(equation_parts) + " = 0"
        if equation.startswith("+ "):
            equation = equation[2:]
        return equation

    def predict_linear(self, X):
        result = []
        i = 0
        while i < len(X):
            result.append(dot(self.weights, X[i]) + self.bias)
            i += 1
        return result

    def predict(self, X):
        result = []
        i = 0
        while i < len(X):
            result.append(step_function(dot(self.weights, X[i]) + self.bias))
            i += 1
        return result

    def plot_epoch(self, X, y, epoch):
        plt.figure(figsize=(8,6))
        class0_x, class0_y, class1_x, class1_y = [], [], [], []
        i = 0
        while i < len(X):
            if y[i] == 0:
                class0_x.append(X[i][0]); class0_y.append(X[i][1])
            else:
                class1_x.append(X[i][0]); class1_y.append(X[i][1])
            i += 1

        plt.scatter(class0_x, class0_y, c='red', label='Class 0')
        plt.scatter(class1_x, class1_y, c='blue', label='Class 1')

        if abs(self.weights[0]) > self.epsilon or abs(self.weights[1]) > self.epsilon:
            x_min = min([X[i][0] for i in range(len(X))]) - 10
            x_max = max([X[i][0] for i in range(len(X))]) + 10
            x_vals, y_vals = [], []
            step = 0
            while step <= 100:
                x_now = x_min + step * (x_max - x_min)/100
                if abs(self.weights[1]) > self.epsilon:
                    y_now = -(self.weights[0]*x_now + self.bias)/self.weights[1]
                    y_vals.append(y_now); x_vals.append(x_now)
                step += 1
            plt.plot(x_vals, y_vals, 'g-', linewidth=2, label='Decision Boundary')

        plt.xlabel('X1'); plt.ylabel('X2')
        plt.title('Epoch '+str(epoch))
        plt.grid(True); plt.legend()
        plt.show()

    def predict_and_explain(self, X_train, y_train, test_points):
        linear_values = self.predict_linear(test_points)
        predictions = self.predict(test_points)
        explanations = []
        i = 0
        while i < len(test_points):
            val = linear_values[i]; pred = predictions[i]
            if abs(val) < self.epsilon:
                reason = "On the line"
            elif val > 0:
                reason = "Above the line → Class 1"
            else:
                reason = "Below the line → Class 0"
            explanations.append(reason); i += 1

        print("\nPrediction Results:")
        print("="*50)
        i = 0
        while i < len(test_points):
            print("Point", i+1, ":", test_points[i])
            print(" Linear Value:", round(linear_values[i],3))
            print(" Predicted Class:", predictions[i])
            print(" Reason:", explanations[i])
            print("-"*50); i += 1

        plt.figure(figsize=(10,7))
        i = 0
        while i < len(X_train):
            if y_train[i] == 0:
                plt.scatter(X_train[i][0], X_train[i][1], c='red', alpha=0.6)
            else:
                plt.scatter(X_train[i][0], X_train[i][1], c='blue', alpha=0.6)
            i += 1

        if abs(self.weights[0]) > self.epsilon or abs(self.weights[1]) > self.epsilon:
            x_min = min([X_train[i][0] for i in range(len(X_train))]+[test_points[i][0] for i in range(len(test_points))]) - 10
            x_max = max([X_train[i][0] for i in range(len(X_train))]+[test_points[i][0] for i in range(len(test_points))]) + 10
            x_vals, y_vals = [], []
            step = 0
            while step <= 100:
                x_now = x_min + step * (x_max - x_min)/100
                if abs(self.weights[1]) > self.epsilon:
                    y_now = -(self.weights[0]*x_now + self.bias)/self.weights[1]
                    x_vals.append(x_now); y_vals.append(y_now)
                step += 1
            plt.plot(x_vals, y_vals, 'g-', linewidth=2)

        markers = ['*','s','^','D','v','<','>','p']
        i = 0
        while i < len(test_points):
            color = 'blue' if predictions[i] == 1 else 'red'
            plt.scatter(test_points[i][0], test_points[i][1], c=color, marker=markers[i%len(markers)], s=200, edgecolors='black', linewidths=2, label="Test "+str(i+1)+" ("+explanations[i]+")")
            i += 1

        plt.xlabel('X1'); plt.ylabel('X2')
        plt.title('Test Points Classification with Explanations')
        plt.grid(True); plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')
        plt.show()


# ---- MAIN ----
X, y, true_w, true_b, df = generate_random_dataset()
print("True separating line: "+str(round(true_w[0],3))+"*x1 + "+str(round(true_w[1],3))+"*x2 + "+str(round(true_b,3))+" = 0")

# Show dataset as a table
print("\nGenerated Dataset Table:\n")
print(df.head(20))  # show first 20 samples for readability

perceptron = EnhancedPerceptron(lr=0.1, epochs=50)
perceptron.fit(X, y)
print("\nTraining completed.")
print("Final weights:", perceptron.weights)
print("Final bias:", perceptron.bias)

test_points = [[10,-20], [-30,40], [25,15], [0,0], [-10,-30], [40,-10]]
perceptron.predict_and_explain(X, y, test_points)